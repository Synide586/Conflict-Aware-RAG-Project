Task:
Using the provided annotations, generate the final expected_response object that gives a grounded, citation-linked answer or a justified abstention.

Inputs (exactly these are provided to you):

- query: {query}
- retrieved_docs: {retrieved_docs}
- per_doc_notes: {per_doc_notes}
- conflict_type: {conflict_type}
- conflict_reason: {conflict_reason}
- answerable_under_evidence: {answerable_under_evidence}
- gold_answer: {gold_answer}

Decision rules:

1) If answerable_under_evidence = true and at least one document verdict ∈ {"supports","partially supports"}:
   - expected_response.abstain = false
   - expected_response.answer = concise, evidence-grounded, and **multi-sentence (2–3 sentences)** with distributed [dX] citations.
   - If the query is a simple fact-retrieval question (e.g., "Who plays...", "When was...", "Where is...") and the evidence is fully consistent (No Conflict), expand the answer to 3–4 sentences covering identification, corroboration, and overall consistency, each supported by citations.
   - When multiple documents provide the same supporting fact, **retain all “supports” and “partially supports” doc_ids in the evidence array** (ordered high-cred first) rather than reducing them arbitrarily; this ensures dataset completeness and traceability.
   - expected_response.evidence = cited doc_ids (high-cred first)
   - expected_response.abstain_reason = null
   - If gold_answer is provided, ensure semantic consistency with it.

2) If answerable_under_evidence = false or all docs are "irrelevant":
   - expected_response.abstain = true
   - expected_response.answer = "CANNOT ANSWER, INSUFFICIENT EVIDENCE"
   - expected_response.evidence = []
   - expected_response.abstain_reason = concise justification naming doc_ids (e.g., “All documents d1–d9 are irrelevant or insufficient to answer the query.”)

3) You must explicitly evaluate every retrieved document in the <think> block.  
   Omitting or grouping docs will be treated as an error.  
   Abstain only if *none* of the docs support or partially support the query.

4) Apply the EXPECTED BEHAVIOR RULES corresponding to the given conflict_type for tone and reasoning structure.

5) Prefer high-credibility sources first in both reasoning and citations.  
   If low-cred sources contribute unique details, cite them secondarily and explain their relevance.

6) Return exactly one JSON object following the schema given in the system prompt.  

7) Ensure the <think> block is syntactically correct and reflects every document’s contribution.

Begin now.